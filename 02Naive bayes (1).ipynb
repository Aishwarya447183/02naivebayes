{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f2a39a-6784-409c-8cd2-851001d4bb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q1.\n",
    "\n",
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem.\n",
    "\n",
    "Let's denote the following events:\n",
    "A: Employee uses the health insurance plan\n",
    "B: Employee is a smoker\n",
    "\n",
    "We are given:\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "P(B|A) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find:\n",
    "P(B|A) = ?\n",
    "\n",
    "According to Bayes' theorem:\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "We need to calculate P(A|B), P(B), and P(A).\n",
    "\n",
    "P(A|B) is the probability that an employee uses the health insurance plan given that they are a smoker. This information is not provided in the given data, so we cannot calculate it directly.\n",
    "\n",
    "P(B) is the probability that an employee is a smoker, which we don't have either.\n",
    "\n",
    "However, we can still calculate P(A) using the given information:\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "\n",
    "Using this information, we can calculate P(B|A) as follows:\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "= (P(B and A) / P(A)) * P(B) / P(A)\n",
    "= P(B and A) / P(A)\n",
    "\n",
    "We cannot directly calculate P(B and A) from the given data, so we cannot determine the exact probability that an employee is a smoker given that they use the health insurance plan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b189563-736c-454c-bba5-b9c5f0a11975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q2.\n",
    "\n",
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the nature of the data they are designed to handle. Here's a breakdown of the differences:\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Data type: Bernoulli Naive Bayes is suitable for binary data, where features represent the presence or absence of specific attributes.\n",
    "Feature representation: It assumes that the features are binary variables, typically represented as 0 (absence) or 1 (presence).\n",
    "Probability estimation: Bernoulli Naive Bayes estimates the probabilities of feature occurrences in each class by counting the presence or absence of features.\n",
    "Example: It is commonly used for text classification tasks where the presence or absence of specific words is considered.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Data type: Multinomial Naive Bayes is suitable for discrete data, typically represented as word frequencies or counts.\n",
    "Feature representation: It assumes that the features follow a multinomial distribution, often represented as integer counts.\n",
    "Probability estimation: Multinomial Naive Bayes estimates the probabilities of feature occurrences in each class by counting the frequencies or occurrences of features.\n",
    "Example: It is often used in document classification tasks where the frequency of words in a document is considered.\n",
    "In summary, Bernoulli Naive Bayes is suited for binary data and focuses on the presence or absence of features, while Multinomial Naive Bayes is suitable for discrete data and considers the frequencies or counts of features. The choice between the two variants depends on the nature of the data and how the features are represented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bf48fb-1165-4699-bc88-d06b63bd61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q3.\n",
    "\n",
    "Bernoulli Naive Bayes handles missing values by considering them as a separate category or treating them as a specific value in the feature representation. Here are two common approaches to dealing with missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "Separate category approach:\n",
    "\n",
    "Missing values are treated as a separate category or a distinct value in the feature representation.\n",
    "During the training phase, the presence or absence of the feature is recorded separately for missing values.\n",
    "During the prediction phase, if a feature value is missing, it is treated as the separate category and considered in the calculation of probabilities.\n",
    "This approach allows the model to handle missing values explicitly and make predictions based on the available information.\n",
    "Binary imputation approach:\n",
    "\n",
    "Missing values are imputed with a specific binary value, such as 0 or 1, indicating the presence or absence of the feature.\n",
    "During the training phase, missing values are replaced with the imputed binary value in the feature representation.\n",
    "During the prediction phase, if a feature value is missing, it is replaced with the imputed binary value.\n",
    "This approach assumes that missing values have the same impact on the classification outcome as the presence or absence of the feature.\n",
    "The choice of approach depends on the specific dataset and the characteristics of the missing values. It is important to consider the potential impact of missing values and the assumptions made by the chosen approach. It is also recommended to evaluate the performance of the classifier with and without imputation to assess the impact of missing values on the model's predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6f3d7-3420-49ab-b28f-a92d8153fbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q4.\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. Although the term \"Gaussian\" implies a Gaussian (normal) distribution for the features, it refers to the assumption that each class follows a Gaussian distribution, not necessarily the individual features themselves.\n",
    "\n",
    "In the case of multi-class classification, Gaussian Naive Bayes estimates the class-conditional probability distributions using the Gaussian distribution assumption for each class. It calculates the mean and standard deviation of the features for each class and uses them to model the probability distributions.\n",
    "\n",
    "During the prediction phase, Gaussian Naive Bayes applies Bayes' theorem to calculate the posterior probabilities of each class given the observed feature values. The class with the highest posterior probability is then selected as the predicted class for the input instance.\n",
    "\n",
    "Although Gaussian Naive Bayes is commonly used for binary and continuous data, it can be extended to handle multi-class classification problems by applying the aforementioned principles. However, it's important to note that Gaussian Naive Bayes assumes independence between features, which might not hold in all cases. Therefore, it's recommended to assess the validity of this assumption and evaluate the classifier's performance on the specific multi-class dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52f3b5-dad9-4ecf-bb97-11f5a983ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
